{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf47b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import re\n",
    "import os\n",
    "import sqlite3\n",
    "import time # 페이지 간 지연 시간을 위해 time 모듈 추가\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8f31ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = requests.get(\"https://finance.naver.com/news/mainnews.naver?date=2024-10-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a250b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''CREATE TABLE IF NOT EXISTS NEWS(ID INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        TITLE TEXT,\n",
    "        CONTENTS TEXT,\n",
    "        DATETIME TEXT,\n",
    "        IMAGE  TEXT,\n",
    "        COMPANY TEXT\n",
    "        )\n",
    "        ''' \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456d0d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_query = '''DROP TABLE NEWS'''\n",
    "conn = sqlite3.connect('my_database.db')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(drop_query)\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8004bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query='''CREATE TABLE IF NOT EXISTS NEWS(\n",
    "    ID INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    TITLE VARCHAR(255),\n",
    "    CONTENTS VARCHAR(255),\n",
    "    NEWSDAY VARCHAR(255),\n",
    "    IMAGE VARCHAR(255),\n",
    "    COMPANY VARCHAR(255)\n",
    ")''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1755532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "# MySQL 서버에 연결\n",
    "con = pymysql.connect(host='localhost', user='root', password='1234',  db='my_database',\n",
    " charset='utf8') # 한글처리 (charset = 'utf8’)\n",
    "# 커서 생성\n",
    "cursor = con.cursor()\n",
    "sql_query = '''CREATE TABLE IF NOT EXISTS NEWS(\n",
    "ID INT PRIMARY KEY AUTO_INCREMENT,  TITLE VARCHAR(255),\n",
    "    CONTENTS VARCHAR(255),\n",
    "    NEWSDAY VARCHAR(255),\n",
    "    IMAGE VARCHAR(255),\n",
    "    COMPANY VARCHAR(255))'''\n",
    "cursor.execute(sql_query)\n",
    "cursor.close()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bc9246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 전역 설정 변수 ---\n",
    "DB_HOST = 'localhost'\n",
    "DB_USER = 'root'\n",
    "DB_PASSWORD = '1234'\n",
    "DB_NAME = 'my_database'\n",
    "IMAGE_DIR = r'C:\\gitmain\\scrap\\images2' # Raw 문자열로 경로 지정\n",
    "# URL 변경: 이제 페이지 번호는 loop에서 추가됩니다.\n",
    "NEWS_BASE_URL = \"https://finance.naver.com/news/mainnews.naver?date==\"\n",
    "NUM_PAGES_TO_SCRAPE = 5 # 수집할 페이지 수2024-10-25&page\n",
    "FILENAME_PATTERN = r'[\\\\/:\"*?<>|]' # 파일명에 사용할 수 없는 문자 패턴\n",
    "PAGE_SCRAPE_DELAY = 2 # 각 페이지 스크래핑 후 지연 시간 (초)\n",
    "\n",
    "# --- 데이터베이스 관련 함수 ---\n",
    "\n",
    "def connect_db():\n",
    "    \"\"\"MySQL 데이터베이스에 연결하고 연결 객체와 커서 객체를 반환합니다.\"\"\"\n",
    "    db_con = pymysql.connect(host=DB_HOST,\n",
    "                             user=DB_USER,\n",
    "                             password=DB_PASSWORD,\n",
    "                             db=DB_NAME,)\n",
    "    db_cursor = db_con.cursor()\n",
    "    print(\"MySQL 데이터베이스에 성공적으로 연결되었습니다.\")\n",
    "    return db_con, db_cursor\n",
    "\n",
    "def save_news_to_db(news_data, db_cursor, db_con):\n",
    "    \"\"\"수집된 뉴스 정보를 데이터베이스에 삽입합니다.\"\"\"\n",
    "    news_title, news_content, news_datetime, news_image_local_path, news_company = news_data\n",
    "    insert_news_query = '''\n",
    "    INSERT INTO NEWS (TITLE, CONTENTS, NEWSDAY, IMAGE, COMPANY)\n",
    "    VALUES (%s, %s, %s, %s, %s)\n",
    "    '''\n",
    "    db_cursor.execute(insert_news_query, (news_title, news_content, news_datetime, news_image_local_path, news_company))\n",
    "    db_con.commit()\n",
    "    print(f\"'{news_title}' 뉴스 정보 데이터베이스 저장 성공.\")\n",
    "\n",
    "# --- 파일 시스템 관련 함수 ---\n",
    "\n",
    "def setup_image_directory(directory_path):\n",
    "    \"\"\"이미지 저장 디렉토리를 생성합니다.\"\"\"\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "        print(f\"이미지 저장 디렉토리 생성: {directory_path}\")\n",
    "\n",
    "# --- 스크래핑 관련 함수 ---\n",
    "\n",
    "def scrape_news_page(url):\n",
    "    \"\"\"지정된 URL에서 뉴스 페이지를 스크래핑하고 BeautifulSoup 객체를 반환합니다.\"\"\"\n",
    "    news_response = requests.get(url) # headers 제거\n",
    "    news_response.raise_for_status() # HTTP 오류 발생 시 예외 발생\n",
    "    print(f\"뉴스 페이지에 성공적으로 연결되었습니다: {url}\")\n",
    "    return BeautifulSoup(news_response.content, 'html.parser')\n",
    "\n",
    "def download_and_save_image(full_image_url, news_title, index, image_dir, pattern):\n",
    "    \"\"\"이미지를 다운로드하고 로컬에 저장한 후 저장 경로를 반환합니다.\"\"\"\n",
    "    news_image_local_path = \"\"\n",
    "    image_response = requests.get(full_image_url) \n",
    "    image_response.raise_for_status() # HTTP 오류 발생 시 예외\n",
    "    \n",
    "    content_type = image_response.headers.get('Content-Type', '')\n",
    "    if 'image' not in content_type:\n",
    "        print(f\"경고: '{news_title}'의 URL에서 이미지 대신 다른 콘텐츠({content_type})를 받았습니다. 로컬 저장 스킵.\")\n",
    "    else:\n",
    "        img = Image.open(BytesIO(image_response.content))\n",
    "        image_filename = re.sub(pattern, '', news_title)\n",
    "        if not image_filename:\n",
    "            image_filename = f\"untitled_news_{index}\"\n",
    "        save_path = os.path.join(image_dir, image_filename + \".png\")\n",
    "        img.save(save_path)\n",
    "        print(f\"이미지 로컬 저장 성공: {save_path}\")\n",
    "        news_image_local_path = save_path\n",
    "    return news_image_local_path\n",
    "\n",
    "def parse_news_item(item_soup, index):\n",
    "    \"\"\"하나의 뉴스 아이템(BeautifulSoup 객체)에서 정보를 파싱합니다.\"\"\"\n",
    "    title_tag = item_soup.select_one('dl > dd.articleSubject > a')\n",
    "    image_tag = item_soup.select_one('dl > dt > a > img')\n",
    "    summary_tag = item_soup.select_one('dl > dd.articleSummary')\n",
    "    date_tag = item_soup.select_one('dl > dd.articleSummary > span.wdate')\n",
    "    company_tag = item_soup.select_one('dl > dd.articleSummary > span.press')\n",
    "\n",
    "    news_title = \"\"\n",
    "    news_content = \"\"\n",
    "    news_datetime = \"\"\n",
    "    news_image_local_path = \"\"\n",
    "    news_company = \"\"\n",
    "\n",
    "    if title_tag:\n",
    "        news_title = title_tag.text.strip()\n",
    "    else:\n",
    "        print(f\"경고: {index+1}번째 뉴스 아이템에서 제목을 찾을 수 없습니다.\")\n",
    "        return None # 제목이 없으면 이 아이템은 건너김\n",
    "\n",
    "    if image_tag and image_tag.get('src'):\n",
    "        image_src = image_tag.get('src')\n",
    "        if image_src.startswith('//'):\n",
    "            full_image_url = \"https:\" + image_src\n",
    "        else:\n",
    "            full_image_url = image_src\n",
    "\n",
    "        # 이미지 다운로드 및 로컬 저장\n",
    "        news_image_local_path = download_and_save_image(full_image_url, news_title, index, IMAGE_DIR, FILENAME_PATTERN)\n",
    "    else:\n",
    "        print(f\"경고: {index+1}번째 뉴스 '{news_title}'에서 이미지를 찾을 수 없습니다.\")\n",
    "\n",
    "    if date_tag:\n",
    "        news_datetime = date_tag.text.strip()\n",
    "    else:\n",
    "        print(f\"경고: {index+1}번째 뉴스 '{news_title}'에서 날짜를 찾을 수 없습니다.\")\n",
    "\n",
    "    if company_tag:\n",
    "        news_company = company_tag.text.strip()\n",
    "    else:\n",
    "        print(f\"경고: {index+1}번째 뉴스 '{news_title}'에서 회사를 찾을 수 없습니다.\")\n",
    "\n",
    "    if summary_tag:\n",
    "        temp_text = summary_tag.text.strip()\n",
    "        if date_tag:\n",
    "            temp_text = temp_text.replace(date_tag.text.strip(), '').strip()\n",
    "        if company_tag:\n",
    "            temp_text = temp_text.replace(company_tag.text.strip(), '').strip()\n",
    "        news_content = temp_text\n",
    "    else:\n",
    "        print(f\"경고: {index+1}번째 뉴스 '{news_title}'에서 요약/내용을 찾을 수 없습니다.\")\n",
    "\n",
    "    return (news_title, news_content, news_datetime, news_image_local_path, news_company)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e8ee15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySQL 데이터베이스에 성공적으로 연결되었습니다.\n",
      "\n",
      "--- 1 페이지 스크래핑 시작 ---\n",
      "뉴스 페이지에 성공적으로 연결되었습니다: https://finance.naver.com/news/mainnews.naver?date==1\n",
      "현재 페이지(1)에서 수집할 뉴스 기사 수: 0\n",
      "경고: 1 페이지에서 뉴스 아이템을 찾을 수 없습니다. 셀렉터가 변경되었거나 마지막 페이지일 수 있습니다.\n",
      "\n",
      "데이터베이스 연결이 닫혔습니다. 총 0개의 뉴스 기사를 수집했습니다.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"뉴스 스크래핑 및 데이터베이스 저장의 전체 과정을 실행합니다.\"\"\"\n",
    "db_con, db_cursor = connect_db()\n",
    "setup_image_directory(IMAGE_DIR)\n",
    "\n",
    "total_scraped_news = 0\n",
    "for page_num in range(1, NUM_PAGES_TO_SCRAPE + 1):\n",
    "    current_page_url = f\"{NEWS_BASE_URL}{page_num}\"\n",
    "    print(f\"\\n--- {page_num} 페이지 스크래핑 시작 ---\")\n",
    "    soup = scrape_news_page(current_page_url)\n",
    "    news_items = soup.select(\"#contentarea_left > div.mainNewsList._replaceNewsLink > ul > li\")\n",
    "\n",
    "    print(f\"현재 페이지({page_num})에서 수집할 뉴스 기사 수: {len(news_items)}\")\n",
    "    if not news_items:\n",
    "        print(f\"경고: {page_num} 페이지에서 뉴스 아이템을 찾을 수 없습니다. 셀렉터가 변경되었거나 마지막 페이지일 수 있습니다.\")\n",
    "        # 더 이상 뉴스가 없으면 다음 페이지로 진행하지 않음\n",
    "        break\n",
    "\n",
    "    for i, item in enumerate(news_items):\n",
    "        news_data = parse_news_item(item, i)\n",
    "        if news_data: # 뉴스 데이터가 성공적으로 파싱된 경우에만 저장\n",
    "            print(f\"수집 정보: 제목='{news_data[0]}', 날짜='{news_data[2]}', 회사='{news_data[4]}', 이미지경로='{news_data[3]}', 내용='{news_data[1][:50]}...'\")\n",
    "            save_news_to_db(news_data, db_cursor, db_con)\n",
    "            total_scraped_news += 1\n",
    "\n",
    "    # 다음 페이지로 넘어가기 전에 지연 시간 추가 (봇 감지 회피)\n",
    "    if page_num < NUM_PAGES_TO_SCRAPE:\n",
    "        print(f\"{PAGE_SCRAPE_DELAY}초 대기 후 다음 페이지로 이동...\")\n",
    "        time.sleep(PAGE_SCRAPE_DELAY)\n",
    "\n",
    "db_con.close()\n",
    "print(f\"\\n데이터베이스 연결이 닫혔습니다. 총 {total_scraped_news}개의 뉴스 기사를 수집했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4028909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySQL 데이터베이스 연결.\n",
      "데이터가 성공적으로 '.\\news_data.csv' 파일로 내보내졌습니다.\n",
      "DBCLOSE.\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# --- 데이터베이스 연결 설정 (기존 스크래핑 코드와 동일하게 설정) ---\n",
    "DB_HOST = 'localhost'\n",
    "DB_USER = 'root'\n",
    "DB_PASSWORD = '1234'\n",
    "DB_NAME = 'my_database'\n",
    "\n",
    "# --- CSV 파일 저장 경로 설정 ---\n",
    "# 현재 스크립트가 실행되는 디렉토리에 저장됩니다.\n",
    "# 필요하다면 다른 경로를 지정할 수 있습니다.\n",
    "OUTPUT_DIR = '.' # 현재 디렉토리\n",
    "CSV_FILENAME = 'news_data.csv'\n",
    "CSV_FULL_PATH = os.path.join(OUTPUT_DIR, CSV_FILENAME)\n",
    "\n",
    "def export_news_to_csv():\n",
    "    \"\"\"MySQL NEWS 테이블의 데이터를 CSV 파일로 내보냅니다.\"\"\"\n",
    "    # 1. MySQL 데이터베이스 연결\n",
    "    db_con = pymysql.connect(host=DB_HOST,\n",
    "                             user=DB_USER,\n",
    "                             password=DB_PASSWORD,\n",
    "                             db=DB_NAME,\n",
    "                             charset='utf8mb4')\n",
    "    db_cursor = db_con.cursor()\n",
    "    print(\"MySQL 데이터베이스 연결.\")\n",
    "\n",
    "    # 2. NEWS 테이블에서 모든 데이터 조회\n",
    "    select_query = \"SELECT ID, TITLE, CONTENTS, NEWSDAY, IMAGE, COMPANY FROM NEWS\"\n",
    "    db_cursor.execute(select_query)\n",
    "    \n",
    "    # 컬럼 이름 가져오기 (CSV 헤더로 사용)\n",
    "    column_names = [desc[0] for desc in db_cursor.description]\n",
    "    \n",
    "    # 3. CSV 파일로 저장\n",
    "    with open(CSV_FULL_PATH, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        \n",
    "        # 헤더(컬럼 이름) 쓰기\n",
    "        csv_writer.writerow(column_names)\n",
    "        \n",
    "        # 데이터 행 쓰기\n",
    "        for row in db_cursor:\n",
    "            csv_writer.writerow(row)\n",
    "            \n",
    "    print(f\"데이터가 성공적으로 '{CSV_FULL_PATH}' 파일로 내보내졌습니다.\")\n",
    "\n",
    "    db_con.close()\n",
    "    print(\"DBCLOSE.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    export_news_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8691559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터베이스에서 뉴스 데이터를 불러오는 중...\n",
      "MySQL DB연결 성공.\n",
      "데이터베이스 연결이 닫혔습니다.\n",
      "총 280개의 뉴스 데이터를 불러왔습니다.\n",
      "\n",
      "--- 불러온 데이터 미리보기 (상위 20개) ---\n",
      "ID: 1\n",
      "제목: ‘오락가락 SK하닉 리포트’ 모건스탠리, 불공정거래 의혹 쟁점은\n",
      "내용: 금융당국이 모건스탠리를 대상으로 진행하고 있는 검사의 핵심은 불건전 영업행위 여부인 것으로...\n",
      "날짜: 2024-10-25 22:58:15\n",
      "이미지 경로: C:\\gitmain\\scrap\\images2\\‘오락가락 SK하닉 리포트’ 모건스탠리, 불공정거래 의혹 쟁점은.png\n",
      "회사: 파이낸셜뉴스\n",
      "------------------------------\n",
      "ID: 2\n",
      "제목: 고려아연  “이번엔 장내매수 경쟁” 장중 29%대 폭등\n",
      "내용: 최윤범 고려아연(010130) 회장 측과 영풍(000670)·MBK연합의 지분 경쟁이 또다...\n",
      "날짜: 2024-10-25 21:23:10\n",
      "이미지 경로: C:\\gitmain\\scrap\\images2\\고려아연  “이번엔 장내매수 경쟁” 장중 29%대 폭등.png\n",
      "회사: 서울경제\n",
      "------------------------------\n",
      "ID: 3\n",
      "제목: [단독]현대차, '獨 디젤차 벌금' 870억 물어\n",
      "내용: 현대차그룹이 독일 프랑크푸르트 검찰이 진행한 디젤 차량 배출가스 배기량 표시와 관련한 수사...\n",
      "날짜: 2024-10-25 21:10:25\n",
      "이미지 경로: C:\\gitmain\\scrap\\images2\\[단독]현대차, '獨 디젤차 벌금' 870억 물어.png\n",
      "회사: 뉴시스\n",
      "------------------------------\n",
      "ID: 4\n",
      "제목: “찬바람 불길래 믿고 사놨는데”…오를 놈은 따로 있다는 보험주\n",
      "내용: 계약 해지? 돌려줄 준비금 회계기준 바뀌며 더 필요해 소형 보험주 배당여력 타격 삼성생명 ...\n",
      "날짜: 2024-10-25 20:23:07\n",
      "이미지 경로: C:\\gitmain\\scrap\\images2\\“찬바람 불길래 믿고 사놨는데”…오를 놈은 따로 있다는 보험주.png\n",
      "회사: 매일경제\n",
      "------------------------------\n",
      "ID: 5\n",
      "제목: \"왜 우리만 떨어지나\"…네이버·카카오 개미들 '비명'\n",
      "내용: 네이버·카카오를 비롯한 국내 인공지능(AI) 관련주 주가가 올초부터 장기 하락세를 면치 못...\n",
      "날짜: 2024-10-25 20:05:10\n",
      "이미지 경로: C:\\gitmain\\scrap\\images2\\왜 우리만 떨어지나…네이버·카카오 개미들 '비명'.png\n",
      "회사: 한국경제\n",
      "------------------------------\n",
      "ID: 6\n",
      "제목: 외환범죄 80%는 '코인 악용'…국경 넘는 코인 거래 보고 추진\n",
      "내용: 코인 같은 가상자산은 국경을 넘어 거래해도 추적이 되지 않죠? 그렇다 보니 이를 악용해 자...\n",
      "날짜: 2024-10-25 20:05:07\n",
      "이미지 경로: C:\\gitmain\\scrap\\images2\\외환범죄 80%는 '코인 악용'…국경 넘는 코인 거래 보고 추진.png\n",
      "회사: MBN\n",
      "------------------------------\n",
      "ID: 7\n",
      "제목: 30분 만에 동났다…연예인 앞세운 편의점 주류 마케팅\n",
      "내용: 최근 연예인을 앞세운 주류 제품이 젊은 층에 인기죠. 특히 주류 제품의 주요 유통채널로 떠...\n",
      "날짜: 2024-10-25 20:03:07\n",
      "이미지 경로: C:\\gitmain\\scrap\\images2\\30분 만에 동났다…연예인 앞세운 편의점 주류 마케팅.png\n",
      "회사: MBN\n",
      "------------------------------\n",
      "ID: 8\n",
      "제목: 고 이건희 4주기 '삼성 위기론' 속 조용한 추모…이재용 쇄신 카드는?\n",
      "내용: 고 이건희 삼성 선대회장의 4주기를 맞아 이재용 회장 등 삼성 일가가 모인 가운데 추도식이...\n",
      "날짜: 2024-10-25 20:01:07\n",
      "이미지 경로: C:\\gitmain\\scrap\\images2\\고 이건희 4주기 '삼성 위기론' 속 조용한 추모…이재용 쇄신 카드는.png\n",
      "회사: MBN\n",
      "------------------------------\n",
      "ID: 9\n",
      "제목: '실손 서류' 안 떼고 앱으로 간편 청구…참여 병원 확대는 숙제\n",
      "내용: 그동안 실손 보험금을 받으려면 병원에서 이것저것 서류를 떼느라 복잡해서 소액은 청구도 하지...\n",
      "날짜: 2024-10-25 19:59:07\n",
      "이미지 경로: C:\\gitmain\\scrap\\images2\\'실손 서류' 안 떼고 앱으로 간편 청구…참여 병원 확대는 숙제.png\n",
      "회사: MBN\n",
      "------------------------------\n",
      "ID: 10\n",
      "제목: 밸류업 공시 'A+' 받은 KB금융, 10만원선 돌파 '역대 최고가'[핫종목]\n",
      "내용: 금융주가 3분기 호실적 발표에 밸류업 기대감까지 더해지며 강세를 보였다. KB금융은 역대 ...\n",
      "날짜: 2024-10-25 19:17:09\n",
      "이미지 경로: C:\\gitmain\\scrap\\images2\\밸류업 공시 'A+' 받은 KB금융, 10만원선 돌파 '역대 최고가'[핫종목].png\n",
      "회사: 뉴스1\n",
      "------------------------------\n",
      "ID: 11\n",
      "제목: 사업비 2조 '잠실 스포츠·MICE 민자사업' 2026년 '첫 삽' 목표\n",
      "내용: 이 기사는 2024년10월25일 17시59분에 마켓인 프리미엄 콘텐츠로 선공개 되었습니다....\n",
      "날짜: 2024-10-25 19:00:18\n",
      "이미지 경로: C:\\gitmain\\scrap\\images2\\사업비 2조 '잠실 스포츠·MICE 민자사업' 2026년 '첫 삽' 목표.png\n",
      "회사: 이데일리\n",
      "------------------------------\n",
      "ID: 12\n",
      "제목: 흑백요리사로 대박 난 백종원…무려 4900억 ‘잭팟’ 터졌다는데\n",
      "내용: 백종원 대표가 이끄는 외식 프랜차이즈 기업 더본코리아가 기관투자자 수요예측 결과 공모가를 ...\n",
      "날짜: 2024-10-25 18:59:06\n",
      "이미지 경로: C:\\gitmain\\scrap\\images2\\흑백요리사로 대박 난 백종원…무려 4900억 ‘잭팟’ 터졌다는데.png\n",
      "회사: 매일경제\n",
      "------------------------------\n",
      "ID: 13\n",
      "제목: ‘마이크로바이옴’ HEM파마, 일반청약서 증거금 2.7조원\n",
      "내용: 에어레인은 1.4조원 모아 마이크로바이옴 전문 기업 에이치이엠파마가 일반청약에서 경쟁률 6...\n",
      "날짜: 2024-10-25 18:38:12\n",
      "이미지 경로: C:\\gitmain\\scrap\\images2\\‘마이크로바이옴’ HEM파마, 일반청약서 증거금 2.7조원.png\n",
      "회사: 매일경제\n",
      "------------------------------\n",
      "ID: 14\n",
      "제목: '20만닉스' 컴백한 날…청산가치 밑으로 추락한 삼전\n",
      "내용: 삼성전자에 대한 외국인 매도세가 거세지면서 주가가 주당순자산가치(BPS)에 미치지 못하는 ...\n",
      "날짜: 2024-10-25 18:10:20\n",
      "이미지 경로: C:\\gitmain\\scrap\\images2\\'20만닉스' 컴백한 날…청산가치 밑으로 추락한 삼전.png\n",
      "회사: 한국경제\n",
      "------------------------------\n",
      "ID: 15\n",
      "제목: 고려아연 주가 125만원…장중 시총 10위 진입도\n",
      "내용: ▶마켓인사이트 10월 25일 오후 4시 4분 고려아연 주가가 이틀째 폭등하면서 시가총액 상...\n",
      "날짜: 2024-10-25 18:08:18\n",
      "이미지 경로: C:\\gitmain\\scrap\\images2\\고려아연 주가 125만원…장중 시총 10위 진입도.png\n",
      "회사: 한국경제\n",
      "------------------------------\n",
      "ID: 16\n",
      "제목: 兆대어만 4~5개…1월 공모주 '황금어장'\n",
      "내용: ▶마켓인사이트 10월 25일 오후 1시 57분 ‘조(兆) 단위’ 대어들이 줄줄이 내년 1분...\n",
      "날짜: 2024-10-25 18:07:14\n",
      "이미지 경로: C:\\gitmain\\scrap\\images2\\兆대어만 4~5개…1월 공모주 '황금어장'.png\n",
      "회사: 한국경제\n",
      "------------------------------\n",
      "ID: 17\n",
      "제목: SK하닉 목표가 반토막 내더니, 모건스탠리 \"틀렸네\"…1만원 찔끔 상향\n",
      "내용: 모건스탠리가 SK하이닉스(000660)에 대한 단기 예상이 빗나갔다고 인정하며 목표주가를 ...\n",
      "날짜: 2024-10-25 17:57:24\n",
      "이미지 경로: C:\\gitmain\\scrap\\images2\\SK하닉 목표가 반토막 내더니, 모건스탠리 틀렸네…1만원 찔끔 상향.png\n",
      "회사: 뉴스1\n",
      "------------------------------\n",
      "ID: 18\n",
      "제목: 이지스운용, 이규성·강영구 2인 대표체제로 조직 개편\n",
      "내용: 이지스자산운용이 전사 협업 체계를 강화하기 위해 이규성 경영부문 대표, 강영구 운용부문 대...\n",
      "날짜: 2024-10-25 17:54:11\n",
      "이미지 경로: C:\\gitmain\\scrap\\images2\\이지스운용, 이규성·강영구 2인 대표체제로 조직 개편.png\n",
      "회사: 데일리안\n",
      "------------------------------\n",
      "ID: 19\n",
      "제목: \"밸류업 노하우 함께 나누자\" 신한금융, 고객사에 컨설팅\n",
      "내용: '기업 밸류업 컨퍼런스' 개최 100개 상장사 참석해 상담 신한금융이 지난주 상장 고객사 ...\n",
      "날짜: 2024-10-25 17:54:10\n",
      "이미지 경로: C:\\gitmain\\scrap\\images2\\밸류업 노하우 함께 나누자 신한금융, 고객사에 컨설팅.png\n",
      "회사: 매일경제\n",
      "------------------------------\n",
      "ID: 20\n",
      "제목: \"트럼프 당선땐 2억 간다\"… 美대선앞 비트코인 출렁\n",
      "내용: 비트코인 미체결약정 54조 역대 최대 수준으로 늘어 ◆ 가상자산 규제 ◆ 미국 대통령선거를...\n",
      "날짜: 2024-10-25 17:53:06\n",
      "이미지 경로: C:\\gitmain\\scrap\\images2\\트럼프 당선땐 2억 간다… 美대선앞 비트코인 출렁.png\n",
      "회사: 매일경제\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def load_news_from_db():\n",
    "    \"\"\"\n",
    "    MySQL 데이터베이스의 NEWS 테이블에서 모든 뉴스 데이터를 불러와 반환합니다.\n",
    "    \"\"\"\n",
    "    # 1. MySQL 데이터베이스 연결\n",
    "    db_con = pymysql.connect(host=DB_HOST,\n",
    "                             user=DB_USER,\n",
    "                             password=DB_PASSWORD,\n",
    "                             db=DB_NAME,\n",
    "                             charset='utf8mb4')\n",
    "    db_cursor = db_con.cursor(pymysql.cursors.DictCursor) # 컬럼 이름을 키로 하는 딕셔너리 형태로 데이터를 가져오기 위해 DictCursor 사용\n",
    "    print(\"MySQL DB연결 성공.\")\n",
    "\n",
    "    # 2. NEWS 테이블에서 모든 데이터 조회\n",
    "    select_query = \"SELECT ID, TITLE, CONTENTS, NEWSDAY, IMAGE, COMPANY FROM NEWS\"\n",
    "    db_cursor.execute(select_query)\n",
    "    \n",
    "    # 3. 모든 데이터 가져오기\n",
    "    news_data = db_cursor.fetchall()\n",
    "    \n",
    "    # 4. 데이터베이스 연결 닫기\n",
    "    db_con.close()\n",
    "    print(\"데이터베이스 연결이 닫혔습니다.\")\n",
    "\n",
    "    return news_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"데이터베이스에서 뉴스 데이터를 불러오는 중...\")\n",
    "    loaded_news = load_news_from_db()\n",
    "\n",
    "    if loaded_news:\n",
    "        print(f\"총 {len(loaded_news)}개의 뉴스 데이터를 불러왔습니다.\")\n",
    "        print(\"\\n--- 불러온 데이터 미리보기 (상위 20개) ---\")\n",
    "        for i, news_item in enumerate(loaded_news[:20]):\n",
    "            print(f\"ID: {news_item['ID']}\")\n",
    "            print(f\"제목: {news_item['TITLE']}\")\n",
    "            print(f\"내용: {news_item['CONTENTS'][:50]}...\") # 내용이 길 수 있으므로 앞부분만 출력\n",
    "            print(f\"날짜: {news_item['NEWSDAY']}\")\n",
    "            print(f\"이미지 경로: {news_item['IMAGE']}\")\n",
    "            print(f\"회사: {news_item['COMPANY']}\")\n",
    "            print(\"-\" * 30)\n",
    "    else:\n",
    "        print(\"데이터베이스에서 뉴스 데이터를 찾을 수 없습니다.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webcrawl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
